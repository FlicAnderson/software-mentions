# Linking #
- [Description](#linking-task-description)
- [Instructions on how to run the code](#how-to-run-the-linking-code)
- [Evaluation](#linking-evaluation)

## Linking Task description ##
1. We query the following databases, searching for exact matches for plain text sofware mentions in our dataset:
- pypi Index: https://pypi.org/simple/
- Bioconductor Index: https://www.bioconductor.org/packages/release/bioc/
- CRAN Index: https://cran.r-project.org/web/packages/available_packages_by_name.html
- Github API: https://github.com
- Scicrunch API: https://scicrunch.org/resources

2. We normalize the metadata files to a [common schema](#linking-schema).
### Linking Schema
Metadata files are normalized to the following fields:

|Field| Description |
| --- | --- |
| ID | unique ID of software mention (generated by us) |
| software_mention | plain-text software mention |
| mapped_to | value the software_mention is being mapped to |
| source | source of the mapping - eg Bioconductor Index, Github API|
| platform | platform of software_mention - eg npm, pypi, CRAN |
| package_url | URL linking software_mention to source |
| description | description of software_mention |
| homepage_url | homepage_url of software_mention|
| other_urls |other related URLs |
| language | programming_language of software_mention|
| license | software license |
| github_repo | Github repository |
| github_repo_license | Github repository license |
| exact_match | whether or not this mapping was an exact match |
| RRID | RRID for software_mention |
| alternate_RRIDs | other RRIDs for software_mention |
| reference | journal articles linked to software_mention (identified either through DOI, pmid or RRID)|
| parent_org | parent organization for software_mention, retrieved from Scicrunch |
| parent_org_link | link for parent organization for software_mention, retrieved from Scicrunch |
| related_condition | related condition for software_mention, retrieved from Scicrunch |
| funding_agency | funding agency for software_mention, retrieved from Scicrunch|
| relation | other entries software_mention is related to, retrieved from Scicrunch |
| resource_type | type of resource for software_mention, retrieved from Scicrunch |
| keywords | keywords for software_mention, retrieved from Scicrunch|
| scicrunch_synonyms | ynonyms for software_mention, retrieved from Scicrunch|

## How to run the Linking code ##
All the scripts for linking are under the `linker` folder. Here are the instructions for running the code from scratch. 

### Step 1: Setup

#### 1. Add the necessary folders and data <br> 
- This step will add a data folder structure.
```
python initialize.py
```

- Download the input data from the [Dryad Link here](https://s3.console.aws.amazon.com/s3/buckets/software-entity-linking-proj?region=us-west-2&prefix=extracted/). Add the input software_mentions file (e.g. `comm_IDs.tsv`) into the `data/input_files` folder. Do not unzip the file. The scripts assume a .gz extension. 

#### 2. Assign IDs for software mentions <br>
This step will assign IDs to software mentions in the input file. It will also generate a mention2ID.pkl file which contains mappings from mention to an ID.
```
python assign_IDs.py --input-file (your_input_file) --output-file (your_output_file)
```
**Example**: ```python assign_IDs.py --input-file comm.tsv.gz --output-file comm_IDs.tsv.gz``` <br>
Note: the script assumes that input_file is under ```data/input_files```. 

At the end of this step, you should have: 
-  `mention2ID.pkl` file under the `data/intermediate_files` 
- `comm_IDs.tsv` file under `data/input_files`

<hr>

### Step 2: Query databases
This step will link software mentions to the PyPI, CRAN, Bioconductor, Scicrunch and Github repositories. <br>
Here are the instructions if you would like to compute the metadata files from scratch:

**1. Generate keys for Accessing the APIs** <br>

You will need to have a number of access keys.  You can get them for free from
[https://libraries.io](https://libraries.io), [https://github.com](https://github.com), [https://sparc.science](https://sparc.science).  Then create a file keys with the following content:

    export GITHUB_USER = ...
    export GITHUB_TOKEN = ...
    export SCICRUNCH_TOKEN = ...

Source this file as `. keys` or `source keys`

**2. Generate Metadata files** <br>
Generate metadata files from scratch. <br>
Each of the commands below queries the specific database for linking and generating metadata for the software mentions. <br>
There are a number of command-line parameters that can be tuned, more info in the scripts themselves. <br>
Metadata files are normalized to a [common schema](#linking-schema) and saved under `data/metadata_files/normalized`. Raw versions are also saved under `data/metadata_files/raw`.

Note that these scripts can take a long time to run, especially given the large number of mentions in the dataset. In particular, the Github API requests are subjected to a limit/per minute. We recommend parallelizing or using distributed computing. We used a Spark environment to speed up the process. 

```
python bioconductor_linker.py --input-file comm_IDs.tsv.gz --generate-new
python cran_linker.py --input-file comm_IDs.tsv.gz --generate-new
python pypi_linker.py --input-file comm_IDs.tsv.gz --generate-new
python github_linker.py --input-file comm_IDs.tsv.gz --generate-new
python scicrunch_linker.py --input-file comm_IDs.tsv.gz --generate-new
```
 
 **Sanity-checking**
 To make sure that everything works the way it should, you could sanity check the code by running something like: <br>
```python bioconductor_linker.py --input-file comm_IDs.tsv --top-k 40``` <br>
This will only try to link the first 40 mentions, for instance, and should take a fairly short time (minutes). Of course, you can do this for any of the metadata linking scripts.

At the end of this step, you should have: 
- **raw metadata files** saved under the `data/metadata_files/raw` directory. 
  - ```pypi_raw_df.csv```
  - ```cran_raw_df.csv```
  - ```bioconductor_raw_df.csv```
  - ```scicrunch_raw_df.csv```
  - ```github_raw_df.csv```
- **normalized files** (to a [common schema](#linking-schema)) saved under the saved under `data/metadata_files/normalized`.  
  - ```pypi_df.csv```
  - ```cran_df.csv```
  - ```bioconductor_df.csv```
  - ```scicrunch_df.csv```
  - ```github_df.csv```

<hr> 

### Step 3. Create master metadata_file <br> 
Once the individual metadata files are computed, aggregate them together by running: 

```
python generate_metadata_file.py
```
This step also does some post-processing of the individual metadata files. 

At the end of this step, you should have:
- `metadata.csv` saved under the `data/output_files/` directory

<hr>

## Linking evaluation ##
We evaluate the linking algorithm using an expert team of biomedical curators. We ask them to evaluate 50 generated **software-generated** link pairs as one of: correct, incorrect or unclear. <br>
The evaluation file is available as `evaluation_linking.csv` and the script to compute the metrics is `evaluation_linking.py`.
To get the evaluation metrics, run the evaluation script inside the `evaluation` folder:

```
python evaluation_linking.py --linking-evaluation-file `../data/curation/evaluation_linking.csv`
```
