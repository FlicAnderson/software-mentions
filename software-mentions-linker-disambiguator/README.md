# Software Mentions Linking + Disambiguation

## Overall Project Description ##
**Goal**: Produce a high quality dataset of software used in the biomedical literature to facilitate analysis of adoption and impact of open-source scientific software.
We are achieving this by:
1. Extracting plain-text software mentions from the PMC-OA access using the NER Machine Learning Algorithm described at [https://github.com/chanzuckerberg/software-mention-extraction](here), developed by Ivana Williams
2. Linking the software mentions to repositories (eg Github) and generating metadata by querying a number of databases (pypi, Bioconductor, CRAN, Scicrunch, libraries.io API)
3. Disambiguating the software mentions by using string-similarity algorithms
4. Analyzing the output

More detailed descriptions of each step are below. Instructions on how to run the files are under **How to run the code**.
 
### Linking ###
1. We are querying a number of databases, searching for the plain text sofware mentions. We are querying the following:
- pypi Index: https://pypi.org/simple/
- Bioconductor Index: https://www.bioconductor.org/packages/release/bioc/
- CRAN Index: https://cran.r-project.org/web/packages/available_packages_by_name.html
- libraries.io API: https://libraries.io/api
- Github API: https://github.com
- Scicrunch API: https://scicrunch.org/resources

2. We are then normalizing the schemas to the following fields:

|Field| Description |
| --- | --- |
| ID | unique ID of software mention (generated by us) |
| software_mention | plain-text software mention |
| mapped_to | value the software_mention is being mapped to |
| source | source of the mapping - eg Bioconductor Index, Github API|
| platform | platform of software_mention - eg npm, pypi, CRAN |
| package_url | URL linking software_mention to source |
| description | description of software_mention |
| homepage_url | homepage_url of software_mention|
| other_urls |other related URLs |
| language | programming_language of software_mention|
| license | software license |
| github_repo | Github repository |
| github_repo_license | Github repository license |
| exact_match | whether or not this mapping was an exact match |
| RRID | RRID for software_mention |
| alternate_RRIDs | other RRIDs for software_mention |
| reference | journal articles linked to software_mention (identified either through DOI, pmid or RRID)|
| parent_org | parent organization for software_mention, retrieved from Scicrunch |
| parent_org_link | link for parent organization for software_mention, retrieved from Scicrunch |
| related_condition | related condition for software_mention, retrieved from Scicrunch |
| funding_agency | funding agency for software_mention, retrieved from Scicrunch|
| relation | other entries software_mention is related to, retrieved from Scicrunch |
| resource_type | type of resource for software_mention, retrieved from Scicrunch |
| keywords | keywords for software_mention, retrieved from Scicrunch|
| scicrunch_synonyms | ynonyms for software_mention, retrieved from Scicrunch|


### Disambiguation ###

## How to run the code ##

**1. Adding the necessary folders and data** <BR>
This will add a data folder structure.
```
python initialize.py
```
 
Download the input data from the [AWS S3 bucket here](https://s3.console.aws.amazon.com/s3/buckets/software-entity-linking-proj?region=us-west-2&prefix=extracted/) into the `data/input_files` folder. 
 
**2. Generate keys for Accessing the APIs** <br>

Many scripts in this bundle require access keys.  You can get them for free from
[https://libraries.io](https://libraries.io), [https://github.com](https://github.com), [https://sparc.science](https://sparc.science).  Then create a file keys with the following content:

    export GITHUB_USER = ...
    export GITHUB_TOKEN = ...
    export LIBRARIES_API_KEY = ...
    export SCICRUNCH_TOKEN = ...

Source this file as `. keys` or `source keys`

**3. Generate IDs for software mentions** <br>
```
python assign_IDs.py --input-file (your_input_file) --output-file (your_output_file)
```

**Example**: ```python assign_IDs.py --input-file comm.tsv --output-file comm_IDs.tsv``` <br><br>
Note: the script assumes that input_file is under ```data/input_files```. This will save the output_file under the same directory. 

**4. Generate Metadata files** <br>
Each of the commands below queries the specific database for linking and generating metadata for the software mentions. 
There are a number of command-line parameters that can be tuned, more info in the scripts themselves. 

```
python bioconductor_linker.py --input-file comm_IDs.tsv --min-freq=1 --generate-new
python cran_linker.py --input-file comm_IDs.tsv --min-freq=1 --generate-new
python pypi_linker.py --input-file comm_IDs.tsv --min-freq=1 --generate-new
python libraries_io_linker.py --input-file comm_IDs.tsv --min-freq=1 --generate-new
python github_linker.py --input-file comm_IDs.tsv --min-freq=1 --generate-new
python scicrunch_linker.py --input-file comm_IDs.tsv --min-freq=1 --generate-new
```

These files will save the output under ```data/metadata_files```. There will be a raw output saved under ```data/metadata_files/raw``` and a normalized output saved under ```data/metadata_files/normalized```.
 
 **Sanity-checking**
 To make sure that everything works the way it should, you could sanity check the code by running something like: <br>
```python bioconductor_linker.py --input-file comm_IDs.tsv --top-k 40``` <br>
This will only try to link the first 40 mentions, for instance, and should take a fairly short time (minutes). Of course, you can do this for any of the metadata linking scripts.
